---
title: "Final_Proj"
author: "Sam Worley"
date: "8/16/2021"
output: 
  md_document:
    variant: markdown_github
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggridges)
library(mosaic)
library(quantmod)
library(foreach)
```

```{r include=FALSE} 
library(readr)
green <- read_csv("Data/greenbuildings.csv")
my_ggtheme <- function() {

  theme_minimal(base_family = "Asap Condensed") +
    theme(
      panel.grid.minor = element_blank(),
      plot.title = element_text(vjust = -1)
          )
}
```

## Visual Story Telling Part 1: Green Buildings

There are few things I think the "Excel Guru" overlooked in his analysis and how he chose to derive at a decision. The first thing that jumps out at me is that after he removed what outliers he noticed, he looked at the rent comparisons of green buildings vs non-green buildings without any further breakdowns. This has issues because there tends to be a lot of variance in some of the variables we have access to. Take the number of stories a building has into account, the proposed project has 15 stories but as you can see in this graph, the data set has a lot of variance

```{r echo=FALSE, warning=FALSE, message=FALSE}
green %>% ggplot(aes(x = stories)) + 
  geom_histogram(fill = "dark orange") + 
  my_ggtheme() + 
  labs(
    title = "Histogram: Number of Stories",
    caption = "Distribution of the number of stories buildings have in the data set",
    x = "Number of Stories",
    y = "Frequenncy"
  )
```

The graph shows that there are sever buildings in this data set with more than 30 stories which is double the number of stories of the proposed buildings. This can impose bias in our data set as different sized buildings might have different trends in energy consumption or other costs. 

We also see that there is significant difference in age of buildings, which is problematic because of the advances in technology in recent years which old buildings may not have had access to.
```{r hist-building-age, include = FALSE}
green %>% ggplot(aes(x = age)) + 
  geom_histogram(fill = "dark orange") + 
  labs(
    title = "Histogram: Building Age",
    y = "Frequency",
    x = "Building Age",
    caption = 'Distribution of Age in the given dataset'
  ) + 
  my_ggtheme()
```

The nice thing about the variance in the data set is that we slice the data to focus on buildings in the data set similar to the proposed constructions project. Taking into account the variables we know, we can filter the data to match the specifications we have by only looking at buildings with between 10 and 20 stories, between 200,000 and 300,000 square feet, and less than 10 years old. This gives us a better comparison because it might remove some of the underlying confounding variables that the "Exel guru" did not take into account and give us a better understanding between the cost and benefit of building eco or not.  

```{r filter-building, include=FALSE, echo=FALSE}
green <- green %>% 
  filter(
    age <= 10,
    size >= 200000 & size <= 300000,
    stories >= 10 & stories <= 20
  ) %>% mutate(
    building_type = ifelse(green_rating == 1, "Green", "Non-Green"),
    diff_compare_to_cluster = Rent - cluster_rent
  )
```

Once we filter the data, we are left with 52 buildings to analyze. Starting by looking at the distribution of rent between green buildings and non-green buildings

```{r geom-ridges, echo=FALSE}
library(ggridges)
green %>% ggplot(aes(x = Rent, y = building_type)) + 
  geom_density_ridges() + 
  my_ggtheme() + 
  labs(
    title = "Distribution of Rent: Green vs Non-Green Buildings",
    x = "Rent per Square Foot per Year ($)",
    y = "Building Type"
  ) + scale_color_manual(values = c("dark green", "grey"))
```

Here it actually looks like the Green buildings have higher revenue streams than non-green buildings but this doesn't take a building's cluster into account which is a better comparison. 


```{r scatter, echo=FALSE}
green %>% ggplot(aes(x = cluster_rent, y = diff_compare_to_cluster)) + 
  geom_point(aes(color = building_type)) +
  my_ggtheme() + 
  labs(
    title = "How Buildings Compare to Cluster Average Rent",
    y = "Difference in Rent Compared to Cluster Average",
    x = "Average Building Rent in Cluster",
    caption = "Rent is yearly income per square foot. Only looking at buildings less than 10 years old"
  ) + scale_color_manual(values = c("dark green", "grey"))

```

In this graph we see that while in most instances, green buildings have higher revenue per square foot than buildings in the same cluster, there are instances where the green buildings have rent prices lower than the cluster average. This further discredits the "Excel Guru's" initial because it is not so cut and dry. To really answer this question more data must be taken into account such as which specific cluster Austin falls into so as there can be better informed decision making into the actual going price for rent for green buildings compared to the average. Plus then you can take into account the number of energy days needed and compute the exact energy costs of the building to make a more accurate decision. 


## Visual Story Telling Part 2: Flights at ABIA

### Understanding Delays and Where they Come From

```{r atx-data, include=FALSE, warning=FALSE, message=FALSE}
atx <- read_csv("Data/ABIA.csv")
```

I'm always fascinated (maybe more so annoyed by) flight delays, and whether pilots can actually "make up time in the air" as they claim.  

```{r flight-time-duration , echo=FALSE}
atx %>% ggplot(aes(x = AirTime, y = ArrDelay)) + 
  geom_point(color = "dark orange", alpha = 0.5) +
  my_ggtheme() + 
  labs(
    title = "Can You Make up Time in the Air?",
    subtitle = "Time in Air vs Arrival Delay", 
    y = "Delay (minutes)",
    x = "Time in Air (minutes)"
  )
```

Wow, looks like an overwhelming number of flights have some sort of delay. But is this always because of time in the air? What about delays on the ground? What happens when we take departure delays out of the equation? 

```{r echo=FALSE, warning=FALSE}
atx %>% mutate(
  delay = ArrDelay - DepDelay
) %>% ggplot(aes(x = AirTime, y = delay)) + 
  geom_point(alpha = 0.5, color = "dark orange") + 
  my_ggtheme() + 
  labs(
    title = "Make up time in air?",
    subtitle = "Taking Departure Delays out of the Equation",
    y = "Arrival Delay - Departure Delay",
    x = "Time in Air",
    caption = "Negative values equate to time made up in air, positive values represent time lost in air"
  )
```

This plot doesn't quite make sense, or at least there is more going on here than we originally thought.  
So what's going on there? Any relation to Taxi Out time? 

```{r taxi-out, echo=FALSE}
atx %>% ggplot(aes(x = TaxiOut, y = DepDelay)) + 
  geom_point(color = "dark orange", alpha = 0.5) + 
  my_ggtheme() + labs(
    title = "Taxi Out vs Departure Delays",
    x = "Time to Taxi Out",
    y = "Departure Delay Time"
  )


```

It looks like there isn't the time needed to Taxi out doesn't have an impact on when a plane was set to depart. But does it have an effect on Arrival Delay?

```{r, echo=FALSE, warning=FALSE}
atx %>% ggplot(aes(x = TaxiOut, y = ArrDelay)) + 
  geom_point(alpha = 0.5, color = "dark orange") +
  my_ggtheme() + 
  labs( x = "Time to Taxi to Take off",
        y = "Arrival Delay",
        title = "Does Taxi Time Affect Arrival Delay?")
```
Now we are getting somewhere, this relationship makes sense and may account for some of the confounding variables we saw in the earlier plot.  

What happens when we take both ends of taxi time into account? 

```{r, echo=FALSE, warning=FALSE}
atx %>% mutate(
  taxiTotal = TaxiOut + TaxiIn
) %>% ggplot( aes(x = taxiTotal, y = ArrDelay)) + 
  geom_point(alpha = 0.5, color = "dark orange") +
  my_ggtheme() + 
  labs(
    title = "Taxi In & Out vs Arrival Delay",
    subtitle = "What happens when we take both taxi out of gate, and taxi to gate into account",
    y = "Arrival Delay",
    x = "Taxi In + Tax Out"
  )
```

This still makes sense but if we add `Departure Delay` too? 

```{r, echo=FALSE, warning=FALSE}
atx %>% mutate( 
    ground_delay = TaxiOut + TaxiIn + DepDelay
  ) %>% 
  ggplot(aes(x = ground_delay, y = ArrDelay)) + 
  geom_point(color = "dark orange", fill = 0.5) + 
  my_ggtheme() + 
  labs(
    title = "Ground Delays vs Arrival Delay", 
    subtitle = "Where Ground Delays is Taxi in + Taxi Out + Departure Delay",
    x = "Ground Delays",
    y = "Arrival Delay"
  ) + geom_abline(intercept = 0, slope = 1, color = "purple")
```
There we go! That makes sense! When we add together all three togehter we see a clear relationship with the total Arrival Delay. This plot also shows how time can be made up in the air by looking at the deviation from the purple line which is simply y = x. So now back to the original question, can you make up more time in the air? 

```{r, echo=FALSE, warning = FALSE}
atx %>% mutate(
  ground_delay = TaxiOut + TaxiIn + DepDelay, 
  airDelay = ArrDelay - ground_delay,
  madeUp = ifelse(airDelay < 0, "Yes", "No")
) %>% ggplot(aes(x = AirTime, y = airDelay)) + 
  geom_point(aes(color = madeUp), alpha = 0.5) + 
  my_ggtheme() +
  labs(
    title = "Can you make up time on Longer Flights?",
    subtitle = "Looking at the distribution of time added or removed in the air",
    y = "Time in the Air contribution to Arrival Delay",
    x = "Time in Air"
  )
```
Here is the answer to the question we were looking for. Pilots can in fact make up some time in the air but it largely but it helps the longer the flight is in the air. 


## Portfolio Modeling
In this project I decided I wanted to learn what NOT to do when it comes to trading and see how making simple mistakes can cost you. For that I'm picking three examples of possible poor trading strategies. The same process is going to be repeated for the three funds. 
* Create a sample portfolio
* Gather historical data
* Run 10,000, 20 day simulations of bootstrap sampling following the provided rules. 

So for the first of the 3 portfolios, I wanted to learn what the return would be if you stuck with a single company who puts togehter these EFTs. The first porfolio all comes from Fidelity and 4 of their various EFTs ranging from Technology to Real Estate. I wanted to see how wise it is to spend it all within a single broker. 
```{r portfolio-1, echo=FALSE, warning=FALSE, cache=TRUE, message=FALSE}
rm(list=ls())

myStocks = c("FTEC", "ONEQ", "FHLC", "FREL")
getSymbols(myStocks)
for (ticker in myStocks) {
  expr = paste0(ticker, "a = adjustOHLC(",ticker,")")
  eval(parse(text=expr))
}

all_returns <- cbind(ClCl(FTEC), ClCl(ONEQ), ClCl(FHLC), ClCl(FREL))

all_returns <- as.matrix(na.omit(all_returns)) 
N = nrow(all_returns)

return.today = resample(all_returns, 1, orig.ids=FALSE)

initial_wealth = 100000
my_weights = c(0.25, 0.25, 0.24, 0.25)
holdings = initial_wealth*my_weights
n_days = 20

sim1 = foreach(i=1:10000, .combine="rbind") %do% {
  total_wealth = initial_wealth
  weights = c(0.25, 0.25, 0.25, 0.25)
  holdings = weights * total_wealth
  wealthtracker = rep(0, n_days)
  for (today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# Profit/Loss
mean(sim1[,n_days]) # 101142.2
sd(sim1[,n_days]) # 5312.29
mean(sim1[,n_days] - initial_wealth) # 1142.153
hist(sim1[,n_days]- initial_wealth, breaks=30)

quantile(sim1[,n_days]-initial_wealth, prop=0.05)
```

When choosing only Fidelity EFTs, our bootstrap sample of 10000 simulations showed that on average we expect the starting wealth to go from `$100,000` to `$101,142.2` over the 20 day trading period with a standard deviation of `$5,312.29`.  


The next two portfolios I wanted to look at is to see what happens when you bet on a certain industry to see the risks associated with that and what happens. For the second portfolio I decided to pick 4 EFTs that all are covering the Tech industry. 

```{r portfolio-2, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
rm(list=ls())
myStocks = c("VGT", "FTEC", "SMH", "IYW")
getSymbols(myStocks)
for (ticker in myStocks) {
  expr = paste0(ticker, "a = adjustOHLC(",ticker,")")
  eval(parse(text=expr))
}

all_returns <- cbind(ClCl(VGT), ClCl(FTEC), ClCl(SMH), ClCl(IYW))

all_returns <- as.matrix(na.omit(all_returns)) 
N = nrow(all_returns)

return.today = resample(all_returns, 1, orig.ids=FALSE)

initial_wealth = 100000
my_weights = c(0.25, 0.25, 0.25, 0.25)
holdings = initial_wealth*my_weights
n_days = 20

sim1 = foreach(i=1:10000, .combine="rbind") %do% {
  total_wealth = initial_wealth
  weights = c(0.25, 0.25, 0.25, 0.25)
  holdings = weights * total_wealth
  wealthtracker = rep(0, n_days)
  for (today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# Profit/Loss
mean(sim1[,n_days]) #102106.5
sd(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth) # 2106.514
hist(sim1[,n_days]- initial_wealth, breaks=30)

quantile(sim1[,n_days]-initial_wealth, prop=0.05)
```
When choosing only Tech EFTs, our bootstrap sample of 10000 simulations showed that on average we expect the starting wealth to go from `$100,000` to `$101,195.2` over the 20 day trading period with a standard deviation of `$6,364.85`. This portfolio had a very slightly higher expected return than the first portfolio but there is higher risk associated in the VaR with this porfolio.

The last industry I wanted to test the strategy of solo porfolio investment is the real estate industry. These 4 funds come from 4 different brokers but all cover the real-estate industry.  

```{r portfolio-3-real-estate, echo = FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
myStocks = c("VNQ", "SCHH", "ICF", "FREL")
getSymbols(myStocks)
for (ticker in myStocks) {
  expr = paste0(ticker, "a = adjustOHLC(",ticker,")")
  eval(parse(text=expr))
}

all_returns <- cbind(ClCl(VNQ), ClCl(SCHH), ClCl(ICF), ClCl(FREL))

all_returns <- as.matrix(na.omit(all_returns)) 
N = nrow(all_returns)

return.today = resample(all_returns, 1, orig.ids=FALSE)

initial_wealth = 100000
my_weights = c(0.25, 0.25, 0.25, 0.25)
holdings = initial_wealth*my_weights
n_days = 20

sim1 = foreach(i=1:10000, .combine="rbind") %do% {
  total_wealth = initial_wealth
  weights = c(0.25, 0.25, 0.25, 0.25)
  holdings = weights * total_wealth
  wealthtracker = rep(0, n_days)
  for (today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

# Profit/Loss
mean(sim1[,n_days]) 
sd(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth) 
hist(sim1[,n_days]- initial_wealth, breaks=30)

quantile(sim1[,n_days]-initial_wealth, prop=0.05)
```

This portfolio had the lowest expected outcome across the 10,000 simulations and also one of the highest risks associated with it. And at the 5% risk level some of the worst outcomes possible. 

This exercise made it clear that diversification is key, even when it is limited to one provider. The first portfolio with the differing industry offerings had the highest estimated return on investment compared to the others that focused solely on one industry. 


## Market Segmentation
```{r market_segmentation_load, include=FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
library(corrplot)

marketing <- read_csv("Data/social_marketing.csv")


```

Looking at the social marketing data and seeing the breakdown of different post breakdowns, it seemed interesting to perform some dimensionality reduction to force the grouping of categories together to make different segments. In an attempt to filter out some of the bots that tend to post the adult content, any user who scored in that category was removed from the dataset. Next, to start to process of dimensionality reduction, we looked at first generating 5 dimensions from the original 36 which produced the output you see below. 
```{r PCA, echo = FALSE}
marketing <- marketing %>% filter(
  adult >= 1
)


pc_marketing = prcomp(marketing %>% select(-X1), rank=5, scale=FALSE)

summary(pc_marketing)


```
This is slightly concerning because it only showed us a cumulative proportion of roughly 60% of the data so we then tried to looking at what happened when we used 6 variables of interest. Which gave us the following output. 

```{r echo=FALSE}

pc_marketing = prcomp(marketing %>% select(-X1), rank=6, 
                      scale=FALSE)
summary(pc_marketing)
```
We see the proportion of explanation go up by 7 points moving from 5 dimensions to 6 dimensions which is decent improvement but we were curious to see what these groups look like and how they differ from each other. The following plots where taken for each of our dimension (`PCX`) and shows their top 5 descriptive categories, and bottom 5 descriptive categories. 
```{r echo = FALSE}
library(gridExtra)
my_ggtheme <- function() {

  theme_minimal(base_family = "Asap Condensed") +
    theme(
      panel.grid.minor = element_blank(),
      plot.title = element_text(vjust = -1)
          )
}

loadings = pc_marketing$rotation %>% 
  as.data.frame()  %>% 
  rownames_to_column('Category')

pc1 <- loadings %>% arrange(desc(PC1)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC1) , y = PC1))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC1",
    x = "",
    y =""
  )

pc2 <- loadings %>% arrange(desc(PC2)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC2) , y = PC2))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC2",
    x = "",
    y =""
  )

pc3 <- loadings %>% arrange(desc(PC3)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC3) , y = PC3))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC3",
    x = "",
    y =""
  )

pc4 <- loadings %>% arrange(desc(PC4)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC4) , y = PC4))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC4",
    x = "",
    y =""
  )

pc5 <- loadings %>% arrange(desc(PC5)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC5) , y = PC5))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC5",
    x = "",
    y =""
  )

pc6 <- loadings %>% arrange(desc(PC6)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC6) , y = PC6))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC6",
    x = "",
    y =""
  )

grid.arrange(pc1, pc2, pc3, pc4, pc5, pc6, nrow=3)

#top_n(loadings, n=5, PC1) #%>% 
  #ggplot(aes(x = "Category", y = PC1)) + 
  #geom_bar()

```
```{r, echo=FALSE}
pc1 <- loadings %>% arrange(PC1) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC1) , y = PC1))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC1",
    x = "",
    y =""
  )

pc2 <- loadings %>% arrange((PC2)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC2) , y = PC2))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC2",
    x = "",
    y =""
  )

pc3 <- loadings %>% arrange((PC3)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC3) , y = PC3))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC3",
    x = "",
    y =""
  )

pc4 <- loadings %>% arrange((PC4)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC4) , y = PC4))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC4",
    x = "",
    y =""
  )

pc5 <- loadings %>% arrange((PC5)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC5) , y = PC5))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC5",
    x = "",
    y =""
  )

pc6 <- loadings %>% arrange((PC6)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC6) , y = PC6))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC6",
    x = "",
    y =""
  )

grid.arrange(pc1, pc2, pc3, pc4, pc5, pc6, nrow=3)
```
These plots show a few interesting things. Firstly, that there are a few repeat categories that appear across multiple dimensions like cooking, health and nutrition, and sports and outdoors. And this makes sense because it is a nutrition company so those traits are likely shared across all the followers. It might be interesting to see the same results but removing some of the common attributes like:
* `health_nutrition`
* `cooking`
* `photo_sharing`

```{r, echo=FALSE}

#marketing <- marketing %>% select(-health_nutrition, -cooking, -photo_sharing)

pc_marketing = prcomp(marketing %>% select(-X1), rank=6, 
                      scale=FALSE)
summary(pc_marketing)


loadings = pc_marketing$rotation %>% 
  as.data.frame()  %>% 
  rownames_to_column('Category')

pc1 <- loadings %>% arrange(desc(PC1)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC1) , y = PC1))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC1",
    x = "",
    y =""
  )

pc2 <- loadings %>% arrange(desc(PC2)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC2) , y = PC2))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC2",
    x = "",
    y =""
  )

pc3 <- loadings %>% arrange(desc(PC3)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC3) , y = PC3))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC3",
    x = "",
    y =""
  )

pc4 <- loadings %>% arrange(desc(PC4)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC4) , y = PC4))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC4",
    x = "",
    y =""
  )

pc5 <- loadings %>% arrange(desc(PC5)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC5) , y = PC5))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC5",
    x = "",
    y =""
  )

pc6 <- loadings %>% arrange(desc(PC6)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC6) , y = PC6))+
  geom_col(fill = "dark green") + coord_flip() + my_ggtheme() + labs(
    title = "Top Categories for PC6",
    x = "",
    y =""
  )

grid.arrange(pc1, pc2, pc3, pc4, pc5,  pc6, nrow=3)


pc1 <- loadings %>% arrange(PC1) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC1) , y = PC1))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC1",
    x = "",
    y =""
  )

pc2 <- loadings %>% arrange((PC2)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC2) , y = PC2))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC2",
    x = "",
    y =""
  )

pc3 <- loadings %>% arrange((PC3)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC3) , y = PC3))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC3",
    x = "",
    y =""
  )

pc4 <- loadings %>% arrange((PC4)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC4) , y = PC4))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC4",
    x = "",
    y =""
  )

pc5 <- loadings %>% arrange((PC5)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC5) , y = PC5))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC5",
    x = "",
    y =""
  )

pc6 <- loadings %>% arrange((PC6)) %>% slice(1:5) %>%
  ggplot(aes(x = reorder(Category, PC6) , y = PC6))+
  geom_col(fill = "dark orange") + coord_flip() + my_ggtheme() + labs(
    title = "Bottom Categories for PC6",
    x = "",
    y =""
  )

grid.arrange(pc1, pc2, pc3, pc4, pc5, pc6, nrow=3)

```
The results of this is still surprising. The cumulative proportion explained actually decreased 3 points and we still don't see super clear differences in our groups. 
We can generate a few user personas for the company however that are slightly different and more distinct than the original. `PC5` and `PC6` seem to be reletively distinct and defined compared to the others. `PC5` provides the company with a demographic who is really big into personal fitness, the outdoors, and dating. Likely healthy, fit, singles in their 20s. `PC6` is similar but with a bigger focus on on automotive and online gaming. This groups is could be associated to towards males.

While these two groups are relatively distinct the `PC4` and `PC3` are very similar to each other and don't provide the company much distinction.  

Going forward it would be helpful for the company to peform more feature extraction from Twitter and beyond what their followers are saying but also what their followers are interacting with, and engagement with various other themed campaigns to determine give this segment analysis more data to work with. 

## Author Attribution

```{r author-setup,  message=FALSE, warning = FALSE}
library(tm) 
library(slam)
library(proxy)
library(readtext)
library(tidytext)
library(stringr)
library(reactable)
library(text2vec)
library(magrittr)
```
```{r echo = FALSE, cache=TRUE}
data_train_dr <- system.file("Data/ReutersC50/C50train/")
data_train <- readtext(paste0(data_train_dr, "Data/ReutersC50/C50train/*"),
                       dvsep = "\n")

data_test_dr <- system.file("Data/ReutersC50/C50test/")
data_test <- readtext(paste0(data_train_dr, "Data/ReutersC50/C50test/*"),
                       dvsep = "\n")


Authornames <- as.data.frame(rep(basename(list.dirs("Data/ReutersC50/C50train")), each=50))
Authornames <- Authornames[-(1:50),]


data_train$Author <- as.factor(Authornames)
data_test$Author <- as.factor(Authornames)

head(data_train)
```

So the first step was reading all the files into two different data frames, one for the training set - the other for testing. Each data frame had two columns, the author and text. After that we were then able to start performing text analysis by cleaning the text data. Starting by transforming all the words to lowercase and turning them into to tokens. After that we created a vector matrix for each text document based on our built vocab. From there we then trained a model on 

```{r error=FALSE, message=FALSE, warning=FALSE, cache=TRUE}


# Create Vocab Words
train_tokens <- word_tokenizer(tolower(data_train$text))
it_train = itoken(train_tokens,
                  progressbar = FALSE)
vocab = create_vocabulary(it_train)


vectorizer = vocab_vectorizer(vocab)

dtm_train = create_dtm(it_train, vectorizer)


library(glmnet)
NFOLDS = 3
glmnet_classifier = cv.glmnet(x = dtm_train, y = data_train[["Author"]],
                              family = "multinomial",
                              alpha = 1,
                              type.measure = "class",
                              nfolds = NFOLDS,
                              thresh = 1e-3,
                              maxit = 1e5)

plot(glmnet_classifier)

```
The training model looks like the best lambda value occurs around lambda = 4. 

```{r}
# Process the training data
test_tokens <- word_tokenizer(tolower(data_test$text))
it_test = itoken(test_tokens,
                  progressbar = FALSE)
vocab = create_vocabulary(it_test)


vectorizer = vocab_vectorizer(vocab)
dtm_test = create_dtm(it_test, vectorizer)



#predict(glmnet_classifier, dtm_train, type = "response")
```


## Association Rule Mining

```{r groceries, include=FALSE}
rm(list=ls())
library(igraph)
library(arules)
library(arulesViz)
groceries <- read.transactions("Data/groceries.txt", sep = ",")
```


The idea for finding rules associated within grocery purchases is to start with loose constraints and thresholds and then tighten them based on what rules appear. These are our rules starting by sorting the rules on confidence, support, and then lift.
```{r, warning=FALSE, error=FALSE}
grocery_rules <- apriori(groceries, parameter=list(support=0.0001, confidence=.001, maxlen=5))
rules_conf <- sort(grocery_rules, by = "confidence", decreasing = TRUE)

inspect(head(rules_conf, n=10))
```

```{r, warning=FALSE, error=FALSE}
rules_conf <- sort(grocery_rules, by = "support", decreasing = TRUE)
inspect(head(rules_conf, n=10))

```

```{r}
rules_conf <- sort(grocery_rules, by = "lift", decreaseing = TRUE)
inspect(head(rules_conf, m=10))

```

```{r}
plot(grocery_rules, method="two-key plot")
```


```{r }
tmp <- subset(grocery_rules, subset = confidence > 0.01 & support > 0.005)
plot(tmp)
```

